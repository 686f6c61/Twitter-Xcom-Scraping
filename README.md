# Twitter/X Conversation Scraper ğŸ¦

Este proyecto permite extraer y almacenar conversaciones completas de Twitter/X, incluyendo tweets principales y sus respuestas, con paginaciÃ³n automÃ¡tica hacia atrÃ¡s en el tiempo. Los datos se guardan en formato JSON estructurado para anÃ¡lisis posterior.

## ğŸ“‹ CaracterÃ­sticas

- âœ… ExtracciÃ³n de tweets principales y todas sus respuestas
- âœ… PaginaciÃ³n automÃ¡tica hacia atrÃ¡s en el tiempo mediante scrolling
- âœ… Almacenamiento en formato JSON estructurado
- âœ… Soporte para mÃºltiples conversaciones en batch
- âœ… ExtracciÃ³n de metadatos (username, timestamp, likes, retweets, replies)
- âœ… Logging detallado del progreso
- âœ… Manejo de rate limiting con delays configurables

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.7 o superior
- Chrome o Chromium instalado
- ChromeDriver (se puede instalar automÃ¡ticamente con selenium)

### Pasos de instalaciÃ³n

1. Clonar el repositorio:
```bash
git clone https://github.com/686f6c61/Twitter-Xcom-Scraping.git
cd Twitter-Xcom-Scraping
```

2. Crear un entorno virtual (recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. Instalar dependencias:
```bash
pip install -r requirements.txt
```

4. (Opcional) Configurar archivo de configuraciÃ³n:
```bash
cp config_example.py config.py
# Editar config.py con tus preferencias
```

## ğŸ’» Uso

### Uso bÃ¡sico

```python
from twitter_scraper import TwitterScraper

# Crear instancia del scraper
scraper = TwitterScraper(headless=True, data_dir="data")

try:
    # Extraer una conversaciÃ³n
    tweet_url = "https://twitter.com/username/status/1234567890"
    conversation = scraper.scrape_conversation(tweet_url, max_scrolls=5)
    
    # Guardar datos
    scraper.save_to_json(conversation, "conversation_output")
    
    # Mostrar resumen
    print(f"Respuestas encontradas: {len(conversation['replies'])}")
    
finally:
    scraper.close()
```

### ExtracciÃ³n de mÃºltiples conversaciones

```python
from twitter_scraper import TwitterScraper

scraper = TwitterScraper(headless=True, data_dir="data")

try:
    tweet_urls = [
        "https://twitter.com/user1/status/111111",
        "https://twitter.com/user2/status/222222",
        "https://twitter.com/user3/status/333333",
    ]
    
    # Extraer mÃºltiples conversaciones con delay entre requests
    conversations = scraper.scrape_multiple_conversations(tweet_urls, delay=5)
    
    # Guardar todas las conversaciones
    scraper.save_to_json(conversations, "multiple_conversations")
    
finally:
    scraper.close()
```

### PaginaciÃ³n extendida

Para cargar mÃ¡s respuestas histÃ³ricas, aumenta el parÃ¡metro `max_scrolls`:

```python
# Cargar mÃ¡s respuestas con mÃ¡s scrolls
conversation = scraper.scrape_conversation(tweet_url, max_scrolls=10)
```

## ğŸ“ Estructura del proyecto

```
Twitter-Xcom-Scraping/
â”œâ”€â”€ twitter_scraper.py      # MÃ³dulo principal del scraper
â”œâ”€â”€ example_usage.py        # Ejemplos de uso
â”œâ”€â”€ config_example.py       # ConfiguraciÃ³n de ejemplo
â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
â”œâ”€â”€ README.md              # DocumentaciÃ³n
â”œâ”€â”€ .gitignore             # Archivos ignorados por Git
â””â”€â”€ data/                  # Directorio de salida (creado automÃ¡ticamente)
    â””â”€â”€ *.json            # Archivos JSON con conversaciones extraÃ­das
```

## ğŸ“Š Formato de datos de salida

Los datos se guardan en formato JSON con la siguiente estructura:

```json
{
  "tweet_url": "https://twitter.com/username/status/1234567890",
  "scrape_date": "2024-01-01T12:00:00",
  "main_tweet": {
    "text": "Contenido del tweet principal...",
    "username": "Usuario",
    "timestamp": "2024-01-01T10:00:00",
    "reply_count": "10",
    "retweet_count": "5",
    "like_count": "20",
    "extracted_at": "2024-01-01T12:00:00"
  },
  "replies": [
    {
      "text": "Primera respuesta...",
      "username": "Usuario2",
      "timestamp": "2024-01-01T10:05:00",
      "reply_count": "2",
      "retweet_count": "0",
      "like_count": "3",
      "extracted_at": "2024-01-01T12:00:00"
    }
  ]
}
```

## âš™ï¸ ConfiguraciÃ³n

### ParÃ¡metros del scraper

- `headless`: Ejecutar navegador sin interfaz grÃ¡fica (True/False)
- `data_dir`: Directorio donde guardar los archivos JSON
- `max_scrolls`: NÃºmero de scrolls para cargar mÃ¡s contenido histÃ³rico
- `delay`: Tiempo de espera entre requests (en segundos)

### Variables de configuraciÃ³n (config.py)

```python
MAX_TWEETS_PER_REQUEST = 100
MAX_REPLIES_PER_TWEET = 200
PAGINATION_DELAY = 2
DATA_DIR = "data"
OUTPUT_FORMAT = "json"
HEADLESS_BROWSER = True
```

## ğŸ” Ejemplos

El archivo `example_usage.py` incluye tres ejemplos completos:

1. **Scraping de una conversaciÃ³n Ãºnica**: Extrae un tweet y sus respuestas
2. **Scraping de mÃºltiples conversaciones**: Procesa varias URLs en batch
3. **Scraping con paginaciÃ³n extendida**: Carga mÃ¡s respuestas histÃ³ricas

Para ejecutar los ejemplos:

```bash
python example_usage.py
```

## âš ï¸ Consideraciones importantes

1. **Rate Limiting**: Twitter/X tiene lÃ­mites de tasa. Usa delays apropiados entre requests.
2. **Robots.txt**: Respeta las polÃ­ticas de robots.txt de Twitter.
3. **TÃ©rminos de Servicio**: AsegÃºrate de cumplir con los TOS de Twitter/X.
4. **Uso responsable**: No sobrecargues los servidores de Twitter con requests masivos.
5. **Datos personales**: Ten cuidado con los datos personales extraÃ­dos.

## ğŸ› ï¸ SoluciÃ³n de problemas

### El scraper no encuentra elementos

- Verifica que la URL del tweet sea vÃ¡lida
- Twitter puede haber cambiado sus selectores CSS
- Intenta ejecutar sin `headless=True` para ver quÃ© pasa

### TimeoutException

- Aumenta los tiempos de espera en el cÃ³digo
- Verifica tu conexiÃ³n a internet
- Twitter puede estar bloqueando el acceso

### ChromeDriver issues

```bash
# Instalar webdriver-manager para gestiÃ³n automÃ¡tica
pip install webdriver-manager
```

## ğŸ“ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## âš–ï¸ Disclaimer

Este proyecto es solo para fines educativos y de investigaciÃ³n. Los usuarios son responsables de cumplir con los tÃ©rminos de servicio de Twitter/X y las leyes aplicables sobre scraping y privacidad de datos.