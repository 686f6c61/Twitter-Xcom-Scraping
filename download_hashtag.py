#!/usr/bin/env python3
"""
Script para descargar toda la conversaci√≥n en torno a un hashtag usando Twitter API

Versi√≥n: 0.3
Autor: @hex686f6c61
GitHub: https://github.com/686f6c61/Twitter-Xcom-Scraping
Twitter/X: https://x.com/hex686f6c61

Changelog v0.3:
- Guardado incremental autom√°tico durante la descarga
- Guardado despu√©s de cada p√°gina de tweets
- Guardado cada 5 tweets con respuestas procesados
- Protecci√≥n contra p√©rdida de datos por interrupciones

Changelog v0.2:
- A√±adido filtro de rango de fechas (desde-hasta)
"""

import os
import requests
import json
import time
import signal
import sys
from datetime import datetime
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Variable global para control de interrupci√≥n
interrupted = False

def signal_handler(sig, frame):
    """Manejador para Ctrl+C"""
    global interrupted
    print("\n\nInterrupci√≥n detectada. Finalizando monitoreo...")
    interrupted = True

class TwitterHashtagScraper:
    def __init__(self):
        self.api_key = os.getenv('RAPIDAPI_KEY')
        self.api_host = os.getenv('RAPIDAPI_HOST')

        if not self.api_key or not self.api_host:
            raise ValueError("RAPIDAPI_KEY y RAPIDAPI_HOST deben estar definidos en .env")

        self.base_url = f"https://{self.api_host}/v1"
        self.headers = {
            'X-RapidAPI-Key': self.api_key,
            'X-RapidAPI-Host': self.api_host
        }

        print(f"Conectando a: {self.api_host}")
        print(f"API Key: {self.api_key[:10]}...{self.api_key[-4:]}")
        print()

    def search_tweets(self, query, mode='latest', max_tweets=None, is_hashtag=True, until_date=None, since_date=None, incremental_save=False, partial_filename=None):
        """
        Busca tweets por hashtag o texto

        Args:
            query: El t√©rmino a buscar (hashtag o texto)
            mode: Modo de b√∫squeda ('top', 'latest', 'photos', 'videos')
            max_tweets: N√∫mero m√°ximo de tweets a descargar (None = todos disponibles)
            is_hashtag: Si True, agrega # si no lo tiene. Si False, busca como texto
            until_date: Fecha l√≠mite superior (m√°s reciente) - formato: YYYY-MM-DD
            since_date: Fecha l√≠mite inferior (m√°s antigua) - formato: YYYY-MM-DD
            incremental_save: Si True, guarda despu√©s de cada p√°gina
            partial_filename: Nombre del archivo para guardado incremental

        Returns:
            Lista de tweets
        """
        # Procesar query seg√∫n el tipo de b√∫squeda
        if is_hashtag and not query.startswith('#'):
            search_query = f'#{query}'
        else:
            search_query = query

        all_tweets = []
        cursor = None
        tweet_count = 0
        page_count = 0
        oldest_date = None
        newest_date = None

        # Convertir fechas a timestamp si est√°n presentes
        until_timestamp = None
        since_timestamp = None
        if until_date:
            from datetime import datetime as dt
            until_timestamp = int(dt.strptime(until_date, '%Y-%m-%d').timestamp())
        if since_date:
            from datetime import datetime as dt
            since_timestamp = int(dt.strptime(since_date, '%Y-%m-%d').timestamp())

        print(f"Buscando tweets para: {search_query}")
        print(f"Modo: {mode}")
        if since_date and until_date:
            print(f"Rango de fechas: desde {since_date} hasta {until_date}")
        elif until_date:
            print(f"Hasta fecha: {until_date}")
        elif since_date:
            print(f"Desde fecha: {since_date}")
        print("-" * 50)

        while True:
            # Preparar par√°metros
            params = {
                'query': search_query,
                'mode': mode
            }

            if cursor:
                params['cursor'] = cursor

            try:
                # Hacer la petici√≥n
                response = requests.get(
                    f"{self.base_url}/search/tweets",
                    headers=self.headers,
                    params=params
                )

                response.raise_for_status()
                data = response.json()

                # Extraer tweets (la API devuelve en data.tweets)
                if 'data' in data and 'tweets' in data['data']:
                    tweets = data['data']['tweets']
                    cursor = data['data'].get('cursor')
                else:
                    tweets = data.get('tweets', [])

                if not tweets:
                    print("No se encontraron m√°s tweets.")
                    break

                # Filtrar tweets por rango de fechas si est√° configurado
                filtered_tweets = []
                stop_pagination = False

                page_count += 1
                for tweet in tweets:
                    tweet_date = tweet.get('time_parsed', '')
                    tweet_timestamp = tweet.get('timestamp', 0)

                    # Actualizar fechas m√°s antigua y m√°s nueva
                    if not oldest_date or (tweet_date and tweet_date < oldest_date):
                        oldest_date = tweet_date
                    if not newest_date or (tweet_date and tweet_date > newest_date):
                        newest_date = tweet_date

                    # Verificar si el tweet est√° dentro del rango de fechas
                    if since_timestamp and tweet_timestamp < since_timestamp:
                        # Ya pasamos la fecha inferior, detener paginaci√≥n
                        print(f"\nAlcanzada la fecha l√≠mite inferior: {since_date}")
                        stop_pagination = True
                        break

                    # Filtrar seg√∫n el rango
                    if until_timestamp and tweet_timestamp > until_timestamp:
                        # Tweet m√°s reciente que el l√≠mite superior, saltarlo
                        continue

                    if since_timestamp and tweet_timestamp < since_timestamp:
                        # Tweet m√°s antiguo que el l√≠mite inferior, saltarlo
                        continue

                    # Tweet dentro del rango (o sin filtros)
                    filtered_tweets.append(tweet)

                all_tweets.extend(filtered_tweets)
                tweet_count = len(all_tweets)

                if stop_pagination:
                    print(f"Total descargado: {tweet_count} tweets en el rango especificado")
                    break

                print(f"P√°gina {page_count}: {len(filtered_tweets)} tweets a√±adidos de {len(tweets)} | Total: {tweet_count} tweets | M√°s antiguo: {oldest_date[:10] if oldest_date else 'N/A'}")

                # Guardado incremental despu√©s de cada p√°gina
                if incremental_save and partial_filename and len(filtered_tweets) > 0:
                    partial_data = {
                        'query': query,
                        'search_type': 'hashtag' if is_hashtag else 'text',
                        'mode': mode,
                        'downloaded_at': datetime.now().isoformat(),
                        'status': 'in_progress',
                        'total_main_tweets': tweet_count,
                        'tweets': [{'tweet': t, 'replies': []} for t in all_tweets]
                    }
                    scraping_dir = 'scraping'
                    if not os.path.exists(scraping_dir):
                        os.makedirs(scraping_dir)
                    filepath = os.path.join(scraping_dir, partial_filename)
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(partial_data, f, ensure_ascii=False, indent=2)
                    print(f"  üíæ Guardado incremental: {tweet_count} tweets")

                # Verificar si llegamos al m√°ximo
                if max_tweets and tweet_count >= max_tweets:
                    all_tweets = all_tweets[:max_tweets]
                    print(f"\nAlcanzado el l√≠mite de {max_tweets} tweets")
                    break

                # Cursor ya se extrajo arriba junto con tweets

                if not cursor:
                    print("No hay m√°s p√°ginas disponibles.")
                    break

                # Peque√±a pausa para no saturar la API
                time.sleep(1)

            except requests.exceptions.HTTPError as e:
                print(f"\n‚ùå Error HTTP: {e}")
                print(f"Respuesta: {response.text}")

                if response.status_code == 403:
                    print("\n‚ö†Ô∏è  PROBLEMA DE SUSCRIPCI√ìN:")
                    print("   Tu API key no est√° suscrita a esta API en RapidAPI.")
                    print("   Pasos para solucionarlo:")
                    print("   1. Ve a: https://rapidapi.com/omarmhaimdat/api/twitter-api45")
                    print("   2. Suscr√≠bete a un plan (hay planes gratuitos)")
                    print("   3. Actualiza tu API key en .env si es necesario")

                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
                break

        return all_tweets

    def get_tweet_replies(self, tweet_id):
        """
        Obtiene las respuestas de un tweet espec√≠fico

        Args:
            tweet_id: ID del tweet

        Returns:
            Lista de respuestas
        """
        all_replies = []
        cursor = None

        while True:
            try:
                params = {}
                if cursor:
                    params['cursor'] = cursor

                response = requests.get(
                    f"{self.base_url}/tweets/{tweet_id}/replies",
                    headers=self.headers,
                    params=params
                )

                response.raise_for_status()
                data = response.json()

                replies = data.get('tweets', [])

                if not replies:
                    break

                all_replies.extend(replies)
                cursor = data.get('cursor')

                if not cursor:
                    break

                time.sleep(0.5)

            except Exception as e:
                print(f"Error obteniendo respuestas del tweet {tweet_id}: {e}")
                break

        return all_replies

    def download_full_conversation(self, query, mode='latest', max_tweets=None, include_replies=True, is_hashtag=True, until_date=None, since_date=None, incremental_save=True):
        """
        Descarga la conversaci√≥n completa incluyendo respuestas

        Args:
            query: El t√©rmino a buscar (hashtag o texto)
            mode: Modo de b√∫squeda
            max_tweets: N√∫mero m√°ximo de tweets principales (None = todos disponibles)
            include_replies: Si incluir las respuestas de cada tweet
            is_hashtag: Si True, trata como hashtag. Si False, como texto libre
            until_date: Fecha l√≠mite superior (m√°s reciente) - formato: YYYY-MM-DD
            since_date: Fecha l√≠mite inferior (m√°s antigua) - formato: YYYY-MM-DD
            incremental_save: Si True, guarda progresivamente (por defecto True)

        Returns:
            Diccionario con tweets y respuestas
        """
        # Preparar nombre de archivo para guardado incremental
        query_clean = query.replace('#', '').replace(' ', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        partial_filename = f"{query_clean}_{timestamp}.json"

        # Buscar tweets principales con guardado incremental
        main_tweets = self.search_tweets(query, mode, max_tweets, is_hashtag, until_date, since_date, incremental_save, partial_filename)

        conversation = {
            'query': query,
            'search_type': 'hashtag' if is_hashtag else 'text',
            'mode': mode,
            'downloaded_at': datetime.now().isoformat(),
            'total_main_tweets': len(main_tweets),
            'tweets': []
        }

        # Obtener respuestas si se solicita
        if include_replies:
            print("\n" + "=" * 50)
            print("Descargando respuestas...")
            print("=" * 50)

            for i, tweet in enumerate(main_tweets, 1):
                tweet_data = {
                    'tweet': tweet,
                    'replies': []
                }

                # Obtener respuestas
                tweet_id = tweet.get('id')
                if tweet_id:
                    print(f"\nTweet {i}/{len(main_tweets)} - ID: {tweet_id}")
                    replies = self.get_tweet_replies(tweet_id)
                    tweet_data['replies'] = replies
                    print(f"  Respuestas encontradas: {len(replies)}")

                conversation['tweets'].append(tweet_data)

                # Guardado incremental despu√©s de cada tweet con respuestas
                if incremental_save and (i % 5 == 0 or i == len(main_tweets)):  # Cada 5 tweets o al final
                    conversation['total_replies'] = sum(len(t['replies']) for t in conversation['tweets'])
                    conversation['total_items'] = conversation['total_main_tweets'] + conversation['total_replies']
                    conversation['status'] = 'in_progress' if i < len(main_tweets) else 'completed'

                    scraping_dir = 'scraping'
                    if not os.path.exists(scraping_dir):
                        os.makedirs(scraping_dir)
                    filepath = os.path.join(scraping_dir, partial_filename)
                    with open(filepath, 'w', encoding='utf-8') as f:
                        json.dump(conversation, f, ensure_ascii=False, indent=2)
                    print(f"  üíæ Guardado incremental: {i}/{len(main_tweets)} tweets procesados")
        else:
            conversation['tweets'] = [{'tweet': tweet, 'replies': []} for tweet in main_tweets]

        # Calcular estad√≠sticas finales
        total_replies = sum(len(t['replies']) for t in conversation['tweets'])
        conversation['total_replies'] = total_replies
        conversation['total_items'] = len(main_tweets) + total_replies
        conversation['status'] = 'completed'

        # Guardado final (siempre, incluso si incremental_save est√° activo)
        if incremental_save:
            conversation['incremental_saved'] = True
            conversation['_saved_filename'] = partial_filename  # Marcar el nombre del archivo usado
            scraping_dir = 'scraping'
            filepath = os.path.join(scraping_dir, partial_filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(conversation, f, ensure_ascii=False, indent=2)
            print(f"\n‚úì Guardado final completado: {filepath}")

        return conversation

    def save_to_json(self, data, filename=None):
        """
        Guarda los datos en un archivo JSON

        Args:
            data: Datos a guardar
            filename: Nombre del archivo (opcional)
        """
        # Si ya tiene status=completed y fue guardado incrementalmente, devolver el path existente
        if data.get('status') == 'completed' and data.get('incremental_saved'):
            saved_filename = data.get('_saved_filename')
            if saved_filename:
                filepath_existing = os.path.join('scraping', saved_filename)
                if os.path.exists(filepath_existing):
                    return filepath_existing

        # Crear carpeta scraping si no existe
        scraping_dir = 'scraping'
        if not os.path.exists(scraping_dir):
            os.makedirs(scraping_dir)

        if not filename:
            query = data['query'].replace('#', '').replace(' ', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{query}_{timestamp}.json"

        # Guardar en la carpeta scraping
        filepath = os.path.join(scraping_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\n‚úì Datos guardados en: {filepath}")
        return filepath

    def export_to_csv(self, data, csv_filename=None):
        """
        Exporta los datos a formato CSV

        Args:
            data: Diccionario con los datos
            csv_filename: Nombre del archivo CSV (opcional)
        """
        try:
            import csv

            scraping_dir = 'scraping'
            if not csv_filename:
                query = data['query'].replace('#', '').replace(' ', '_')
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                csv_filename = f"{query}_{timestamp}.csv"

            csv_filepath = os.path.join(scraping_dir, csv_filename)

            # Preparar datos para CSV
            rows = []
            for item in data['tweets']:
                tweet = item['tweet']
                rows.append({
                    'id': tweet.get('id', ''),
                    'fecha': tweet.get('time_parsed', ''),
                    'usuario': tweet.get('username', ''),
                    'nombre': tweet.get('name', ''),
                    'texto': tweet.get('text', ''),
                    'likes': tweet.get('likes', 0),
                    'retweets': tweet.get('retweets', 0),
                    'respuestas': tweet.get('replies', 0),
                    'vistas': tweet.get('views', 0),
                    'es_verificado': tweet.get('is_verified', False),
                    'url': tweet.get('permanent_url', ''),
                    'hashtags': ','.join(tweet.get('hashtags', [])) if tweet.get('hashtags') else '',
                    'num_respuestas_descargadas': len(item.get('replies', []))
                })

            # Escribir CSV
            if rows:
                with open(csv_filepath, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=rows[0].keys())
                    writer.writeheader()
                    writer.writerows(rows)

                print(f"‚úì CSV exportado en: {csv_filepath}")
                return csv_filepath
            else:
                print("‚ö†Ô∏è  No hay datos para exportar a CSV")
                return None

        except Exception as e:
            print(f"‚ùå Error al exportar CSV: {e}")
            return None

    def apply_filters(self, data, min_likes=None, verified_only=False):
        """
        Aplica filtros a los tweets

        Args:
            data: Diccionario con los tweets
            min_likes: M√≠nimo de likes requeridos
            verified_only: Solo usuarios verificados

        Returns:
            Diccionario filtrado
        """
        original_count = len(data['tweets'])
        filtered_tweets = []

        for item in data['tweets']:
            tweet = item['tweet']

            # Filtro por likes
            if min_likes is not None and tweet.get('likes', 0) < min_likes:
                continue

            # Filtro por verificados
            if verified_only:
                is_verified = tweet.get('is_verified', False) or tweet.get('is_blue_verified', False)
                if not is_verified:
                    continue

            filtered_tweets.append(item)

        data['tweets'] = filtered_tweets
        data['total_main_tweets'] = len(filtered_tweets)
        data['total_items'] = len(filtered_tweets) + sum(len(t['replies']) for t in filtered_tweets)

        filtered_count = len(filtered_tweets)
        print(f"\n‚úì Filtros aplicados: {original_count} tweets ‚Üí {filtered_count} tweets")

        return data


def main():
    """Funci√≥n principal"""
    scraper = TwitterHashtagScraper()

    # Configuraci√≥n
    print("\n" + "=" * 70)
    print("T√âRMINOS DE B√öSQUEDA")
    print("=" * 70)
    print("Puedes buscar uno o varios t√©rminos:")
    print("- Un t√©rmino: Python")
    print("- Varios t√©rminos separados por comas: Python, JavaScript, AI")
    print()

    query_input = input("Ingresa el/los t√©rmino(s) a buscar: ").strip()

    # Separar m√∫ltiples t√©rminos por comas
    queries = [q.strip() for q in query_input.split(',') if q.strip()]

    if len(queries) == 0:
        print("‚ùå Error: Debes ingresar al menos un t√©rmino de b√∫squeda")
        return

    if len(queries) > 1:
        print(f"\n‚úì Se buscar√°n {len(queries)} t√©rminos: {', '.join(queries)}")
    else:
        query = queries[0]

    print("\n" + "=" * 70)
    print("TIPO DE B√öSQUEDA")
    print("=" * 70)
    print("1. Hashtag - Agrega # autom√°ticamente (ej: Python ‚Üí #Python)")
    print("2. Texto libre - Busca nombre, frase o palabra (ej: Elon Musk)")

    search_type = input("\nSelecciona el tipo (1-2, default=1): ").strip()
    is_hashtag = search_type != '2'

    print("\n" + "=" * 70)
    print("MODO DE B√öSQUEDA")
    print("=" * 70)
    print("1. Latest (M√°s recientes)")
    print("   ‚Üí Tweets ordenados por fecha, del m√°s nuevo al m√°s antiguo")
    print("   ‚Üí √ötil para ver las √∫ltimas publicaciones sobre un tema")
    print()
    print("2. Top (M√°s populares)")
    print("   ‚Üí Tweets con m√°s likes, retweets y engagement")
    print("   ‚Üí √ötil para ver el contenido m√°s viral o relevante")
    print()
    print("3. Photos (Solo fotos)")
    print("   ‚Üí Solo tweets que contienen im√°genes")
    print("   ‚Üí √ötil para an√°lisis visual o recopilaci√≥n de im√°genes")
    print()
    print("4. Videos (Solo videos)")
    print("   ‚Üí Solo tweets que contienen videos")
    print("   ‚Üí √ötil para recopilar contenido multimedia")

    mode_choice = input("\nSelecciona el modo (1-4, default=1): ").strip()
    mode_map = {'1': 'latest', '2': 'top', '3': 'photos', '4': 'videos'}
    mode = mode_map.get(mode_choice, 'latest')

    print("\n" + "=" * 70)
    max_tweets_input = input("¬øCu√°ntos tweets descargar? (Enter = todos disponibles): ").strip()
    max_tweets = int(max_tweets_input) if max_tweets_input else None

    # Pregunta de rango de fechas
    date_range_input = input("¬øFiltrar por rango de fechas? (s/n, default=n): ").strip().lower()

    until_date = None
    since_date = None

    if date_range_input == 's':
        print("\n   Configura el rango de fechas (formato DD-MM-YYYY)")
        print("   Puedes especificar solo una fecha o ambas:")

        since_date_input = input("   - Desde (fecha m√°s antigua): ").strip()
        until_date_input = input("   - Hasta (fecha m√°s reciente): ").strip()

        # Convertir since_date de DD-MM-YYYY a YYYY-MM-DD
        if since_date_input:
            try:
                day, month, year = since_date_input.split('-')
                since_date = f"{year}-{month}-{day}"
            except:
                print("   ‚ö†Ô∏è  Formato de fecha 'desde' incorrecto, se ignorar√°")
                since_date = None

        # Convertir until_date de DD-MM-YYYY a YYYY-MM-DD
        if until_date_input:
            try:
                day, month, year = until_date_input.split('-')
                until_date = f"{year}-{month}-{day}"
            except:
                print("   ‚ö†Ô∏è  Formato de fecha 'hasta' incorrecto, se ignorar√°")
                until_date = None

    include_replies_input = input("\n¬øIncluir respuestas? (s/n, default=s): ").strip().lower()
    include_replies = include_replies_input != 'n'

    # Opciones avanzadas
    print("\n" + "=" * 70)
    advanced_input = input("¬øConfigurar opciones avanzadas? (s/n, default=n): ").strip().lower()

    # Variables para opciones avanzadas
    export_csv = False
    min_likes = None
    verified_only = False
    monitor_mode = False
    monitor_duration = None

    if advanced_input == 's':
        print("\n" + "=" * 70)
        print("OPCIONES AVANZADAS")
        print("=" * 70)

        # Exportar a CSV
        csv_input = input("\n¬øExportar tambi√©n a CSV? (s/n, default=n): ").strip().lower()
        export_csv = csv_input == 's'

        # Filtro por likes
        min_likes_input = input("Filtrar tweets con m√≠nimo de likes (Enter = sin filtro): ").strip()
        min_likes = None
        if min_likes_input and min_likes_input.isdigit():
            min_likes = int(min_likes_input)

        # Filtro por verificados
        verified_input = input("¬øSolo usuarios verificados? (s/n, default=n): ").strip().lower()
        verified_only = verified_input == 's'

        # Modo monitoreo
        monitor_input = input("\n¬øActivar modo monitoreo continuo? (s/n, default=n): ").strip().lower()
        monitor_mode = monitor_input == 's'

        if monitor_mode:
            print("\nDuraci√≥n del monitoreo:")
            print("1. 10 horas")
            print("2. 24 horas")
            print("3. 2 d√≠as")
            print("4. Hasta detenerlo manualmente (Ctrl+C)")

            duration_choice = input("\nSelecciona duraci√≥n (1-4, default=4): ").strip()
            duration_map = {
                '1': 10 * 3600,      # 10 horas en segundos
                '2': 24 * 3600,      # 24 horas
                '3': 2 * 24 * 3600,  # 2 d√≠as
                '4': None            # Indefinido
            }
            monitor_duration = duration_map.get(duration_choice, None)

            interval_input = input("Intervalo entre b√∫squedas en minutos (default=5): ").strip()
            monitor_interval = int(interval_input) * 60 if interval_input else 300  # 5 minutos por defecto

    print("\n" + "=" * 50)
    print("Iniciando descarga...")
    print("=" * 50)

    # Configurar manejador de se√±ales para Ctrl+C
    signal.signal(signal.SIGINT, signal_handler)

    # Procesar m√∫ltiples b√∫squedas si hay varios t√©rminos
    if len(queries) > 1:
        print("\n" + "=" * 70)
        print(f"PROCESANDO {len(queries)} B√öSQUEDAS")
        print("=" * 70)

        all_results = []

        for idx, query in enumerate(queries, 1):
            print(f"\n{'=' * 70}")
            print(f"B√öSQUEDA {idx}/{len(queries)}: {query}")
            print(f"{'=' * 70}")

            # Modo monitoreo no compatible con m√∫ltiples t√©rminos
            if monitor_mode:
                print("‚ö†Ô∏è  Modo monitoreo no disponible con m√∫ltiples t√©rminos")
                print("   Usando modo de b√∫squeda √∫nica...")

            conversation = scraper.download_full_conversation(
                query=query,
                mode=mode,
                max_tweets=max_tweets,
                include_replies=include_replies,
                is_hashtag=is_hashtag,
                until_date=until_date,
                since_date=since_date
            )

            # Aplicar filtros si est√°n configurados
            if min_likes or verified_only:
                conversation = scraper.apply_filters(conversation, min_likes, verified_only)

            # Guardar resultados
            filename = scraper.save_to_json(conversation)

            # Exportar a CSV si est√° activado
            if export_csv:
                scraper.export_to_csv(conversation)

            all_results.append({
                'query': query,
                'tweets': conversation['total_main_tweets'],
                'replies': conversation['total_replies'],
                'file': filename
            })

            print(f"‚úì Completado: {conversation['total_main_tweets']} tweets, {conversation['total_replies']} respuestas")

        # Resumen final de todas las b√∫squedas
        print("\n" + "=" * 70)
        print("RESUMEN GENERAL")
        print("=" * 70)

        total_tweets = sum(r['tweets'] for r in all_results)
        total_replies = sum(r['replies'] for r in all_results)

        for idx, result in enumerate(all_results, 1):
            print(f"\n{idx}. {result['query']}")
            print(f"   Tweets: {result['tweets']}")
            print(f"   Respuestas: {result['replies']}")
            print(f"   Archivo: {result['file']}")

        print(f"\n{'=' * 70}")
        print(f"TOTAL: {total_tweets} tweets, {total_replies} respuestas")
        print(f"Archivos generados: {len(all_results)}")
        print("=" * 70)

        return

    # Procesar b√∫squeda √∫nica
    query = queries[0]

    # Modo monitoreo
    if monitor_mode:
        print("\n" + "=" * 70)
        print("MODO MONITOREO ACTIVADO")
        print("=" * 70)
        if monitor_duration:
            hours = monitor_duration / 3600
            print(f"Duraci√≥n: {hours} horas")
        else:
            print("Duraci√≥n: Hasta detenerlo manualmente (Ctrl+C)")
        print(f"Intervalo: {monitor_interval / 60} minutos")
        print("=" * 70)

        start_time = time.time()
        iteration = 0
        all_tweet_ids = set()  # Para detectar duplicados

        while not interrupted:
            iteration += 1
            current_time = time.time()

            # Verificar si se alcanz√≥ el l√≠mite de tiempo
            if monitor_duration and (current_time - start_time) >= monitor_duration:
                print(f"\n‚úì Tiempo de monitoreo completado ({monitor_duration / 3600} horas)")
                break

            print(f"\n{'=' * 70}")
            print(f"ITERACI√ìN {iteration} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'=' * 70}")

            # Descargar conversaci√≥n
            conversation = scraper.download_full_conversation(
                query=query,
                mode=mode,
                max_tweets=max_tweets,
                include_replies=include_replies,
                is_hashtag=is_hashtag,
                until_date=until_date,
                since_date=since_date
            )

            # Aplicar filtros si est√°n configurados
            if min_likes or verified_only:
                conversation = scraper.apply_filters(conversation, min_likes, verified_only)

            # Detectar tweets nuevos
            new_tweets = 0
            for item in conversation['tweets']:
                tweet_id = item['tweet'].get('id')
                if tweet_id and tweet_id not in all_tweet_ids:
                    all_tweet_ids.add(tweet_id)
                    new_tweets += 1

            print(f"\n‚úì Tweets nuevos en esta iteraci√≥n: {new_tweets}")
            print(f"‚úì Total de tweets √∫nicos monitorizados: {len(all_tweet_ids)}")

            # Guardar resultados
            filename = scraper.save_to_json(conversation)

            # Exportar a CSV si est√° activado
            if export_csv:
                scraper.export_to_csv(conversation)

            # Esperar hasta la pr√≥xima iteraci√≥n
            if not interrupted:
                print(f"\n‚è≥ Esperando {monitor_interval / 60} minutos hasta la pr√≥xima b√∫squeda...")
                print("   (Presiona Ctrl+C para detener)")

                # Esperar por intervalos de 1 segundo para poder interrumpir r√°pido
                for _ in range(monitor_interval):
                    if interrupted:
                        break
                    time.sleep(1)

        print("\n" + "=" * 70)
        print("MONITOREO FINALIZADO")
        print("=" * 70)
        print(f"Iteraciones completadas: {iteration}")
        print(f"Tweets √∫nicos monitorizados: {len(all_tweet_ids)}")
        print("=" * 70)

    else:
        # Modo normal (una sola extracci√≥n)
        conversation = scraper.download_full_conversation(
            query=query,
            mode=mode,
            max_tweets=max_tweets,
            include_replies=include_replies,
            is_hashtag=is_hashtag,
            until_date=until_date,
            since_date=since_date
        )

        # Aplicar filtros si est√°n configurados
        if min_likes or verified_only:
            conversation = scraper.apply_filters(conversation, min_likes, verified_only)

        # Guardar resultados
        filename = scraper.save_to_json(conversation)

        # Exportar a CSV si est√° activado
        if export_csv:
            scraper.export_to_csv(conversation)

        # Mostrar resumen
        print("\n" + "=" * 50)
        print("RESUMEN")
        print("=" * 50)
        print(f"B√∫squeda: {conversation['query']}")
        print(f"Tipo: {conversation['search_type']}")
        print(f"Tweets principales: {conversation['total_main_tweets']}")
        print(f"Total de respuestas: {conversation['total_replies']}")
        print(f"Total de elementos: {conversation['total_items']}")
        print(f"Archivo JSON: {filename}")
        print("=" * 50)


if __name__ == '__main__':
    main()
